{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Palindrome Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_string(length):\n",
    "    return ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), length))\n",
    "\n",
    "\n",
    "def generate_palindrome(length):\n",
    "    half_length = length // 2\n",
    "    half_string = generate_random_string(half_length)\n",
    "    if length % 2 == 0:\n",
    "        return half_string + half_string[::-1]\n",
    "    else:\n",
    "        return half_string + np.random.choice(list('abcdefghijklmnopqrstuvwxyz')) + half_string[::-1]\n",
    "    \n",
    "\n",
    "def generate_dataset(num_samples, string_lengths):\n",
    "    data = []\n",
    "    num_samples_per_length = num_samples // len(string_lengths)\n",
    "    for length in string_lengths:\n",
    "        for _ in range(num_samples_per_length):\n",
    "            if np.random.rand() > 0.5:\n",
    "                string = generate_random_string(length)\n",
    "                label = 1\n",
    "            else:\n",
    "                string = generate_palindrome(length)\n",
    "                label = 0\n",
    "            data.append([string, label])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Manual Palindrome Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for 'aba':  0.0\n",
      "Output for 'abb':  2.0\n",
      "Output for 'cba':  2.0\n",
      "Output for 'racecar':  0.0\n",
      "Output for 'saippuakivikauppias':  0.0\n"
     ]
    }
   ],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        # Define the special tokens\n",
    "        self.tokens = list(string.ascii_lowercase)\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.tokens)}\n",
    "        self.id_to_token = {i: token for token, i in enumerate(self.tokens)}\n",
    "        self.vocab = set(self.tokens)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_word_length = 200\n",
    "    \n",
    "    def to_matrix(self, word):\n",
    "        rows = len(word)\n",
    "        cols = len(self.vocab)\n",
    "        matrix = torch.zeros((rows, cols), dtype=torch.float32)\n",
    "        for i, char in enumerate(word):\n",
    "            matrix[i, self.token_to_id[char]] = 1.0\n",
    "        return matrix\n",
    "    \n",
    "class PalindromeNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PalindromeNetwork, self).__init__()\n",
    "        self.tokenizer = CustomTokenizer() \n",
    "        self.IPrime = torch.flip(torch.eye(self.tokenizer.max_word_length), [0])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        word_len = len(x)\n",
    "        P = self.tokenizer.to_matrix(x)\n",
    "        d = torch.norm(P - self.IPrime[-1*word_len:, :word_len].matmul(P), p=\"fro\")\n",
    "        return d\n",
    "    \n",
    "net = PalindromeNetwork()\n",
    "\n",
    "print(\"Output for 'aba': \", net(\"aba\").item())\n",
    "print(\"Output for 'abb': \", net(\"abb\").item())\n",
    "print(\"Output for 'cba': \", net(\"cba\").item())\n",
    "print(\"Output for 'racecar': \", net(\"racecar\").item())\n",
    "print(\"Output for 'saippuakivikauppias': \", net(\"saippuakivikauppias\").item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expirement 1: Learning the Reverse Identity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.0275\n",
      "Epoch [2/7], Loss: 0.0088\n",
      "Epoch [3/7], Loss: 0.0087\n",
      "Epoch [4/7], Loss: 0.0087\n",
      "Epoch [5/7], Loss: 0.0087\n",
      "Epoch [6/7], Loss: 0.0086\n",
      "Epoch [7/7], Loss: 0.0086\n"
     ]
    }
   ],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        # Define the special tokens\n",
    "        self.tokens = list(string.ascii_lowercase)\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.tokens)}\n",
    "        self.id_to_token = {i: token for token, i in enumerate(self.tokens)}\n",
    "        self.vocab = set(self.tokens)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_word_length = 7\n",
    "    \n",
    "    def to_matrix(self, word):\n",
    "        rows = len(word)\n",
    "        cols = len(self.vocab)\n",
    "        matrix = torch.zeros((rows, cols), dtype=torch.float32)\n",
    "        for i, char in enumerate(word):\n",
    "            matrix[i, self.token_to_id[char]] = 1.0\n",
    "        return matrix\n",
    "    \n",
    "class PalindromeNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PalindromeNetwork, self).__init__()\n",
    "        self.tokenizer = CustomTokenizer() \n",
    "        self.IPrime = nn.Parameter(torch.zeros(self.tokenizer.max_word_length,self.tokenizer.max_word_length))\n",
    "\n",
    "    def forward(self, P):\n",
    "        word_len = P.shape[0]\n",
    "        d = torch.norm(P - self.IPrime[-1*word_len:, :word_len].matmul(P), p=\"fro\")\n",
    "        d = nn.functional.tanh(d)\n",
    "        return d\n",
    "\n",
    "dataset = generate_dataset(10000, [3,4,5])\n",
    "tokenizer = CustomTokenizer()\n",
    "X = []\n",
    "for row in dataset:\n",
    "    X.append([tokenizer.to_matrix(row[0]), row[1]])\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model = PalindromeNetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 7\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for data in X:\n",
    "        inputs, label = data\n",
    "        label = torch.tensor([label], dtype=torch.float32)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(X):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.46 -0.   -0.    0.    0.54  0.    0.  ]\n",
      " [ 0.15  0.08  0.    0.92 -0.15  0.    0.  ]\n",
      " [-0.03 -0.03  1.    0.03  0.03  0.    0.  ]\n",
      " [ 0.02  1.02 -0.   -0.02 -0.02  0.    0.  ]\n",
      " [ 1.07  0.06  0.   -0.06 -0.07  0.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x313458110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model.IPrime.detach().numpy().round(2))\n",
    "plt.imshow(model.IPrime.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002, grad_fn=<TanhBackward0>)\n",
      "tensor(0.8383, grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(tokenizer.to_matrix(\"abbba\")))\n",
    "print(model(tokenizer.to_matrix(\"abbbba\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expirement 2: Replacing the Subtraction & Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Train Loss: 0.5348, Test Loss: 0.1558\n",
      "Epoch [2/40], Train Loss: 0.1009, Test Loss: 0.0710\n",
      "Epoch [3/40], Train Loss: 0.0755, Test Loss: 0.0615\n",
      "Epoch [4/40], Train Loss: 0.0692, Test Loss: 0.0571\n",
      "Epoch [5/40], Train Loss: 0.0651, Test Loss: 0.0555\n",
      "Epoch [6/40], Train Loss: 0.0621, Test Loss: 0.0546\n",
      "Epoch [7/40], Train Loss: 0.0598, Test Loss: 0.0547\n",
      "Epoch [8/40], Train Loss: 0.0580, Test Loss: 0.0550\n",
      "Epoch [9/40], Train Loss: 0.0568, Test Loss: 0.0552\n",
      "Epoch [10/40], Train Loss: 0.0555, Test Loss: 0.0548\n",
      "Epoch [11/40], Train Loss: 0.0545, Test Loss: 0.0553\n",
      "Epoch [12/40], Train Loss: 0.0537, Test Loss: 0.0552\n",
      "Epoch [13/40], Train Loss: 0.0528, Test Loss: 0.0552\n",
      "Epoch [14/40], Train Loss: 0.0520, Test Loss: 0.0554\n",
      "Epoch [15/40], Train Loss: 0.0513, Test Loss: 0.0552\n",
      "Epoch [16/40], Train Loss: 0.0507, Test Loss: 0.0550\n",
      "Epoch [17/40], Train Loss: 0.0502, Test Loss: 0.0547\n",
      "Epoch [18/40], Train Loss: 0.0497, Test Loss: 0.0541\n",
      "Epoch [19/40], Train Loss: 0.0493, Test Loss: 0.0539\n",
      "Epoch [20/40], Train Loss: 0.0489, Test Loss: 0.0533\n",
      "Epoch [21/40], Train Loss: 0.0485, Test Loss: 0.0531\n",
      "Epoch [22/40], Train Loss: 0.0481, Test Loss: 0.0529\n",
      "Epoch [23/40], Train Loss: 0.0478, Test Loss: 0.0526\n",
      "Epoch [24/40], Train Loss: 0.0474, Test Loss: 0.0524\n",
      "Epoch [25/40], Train Loss: 0.0471, Test Loss: 0.0524\n",
      "Epoch [26/40], Train Loss: 0.0468, Test Loss: 0.0523\n",
      "Epoch [27/40], Train Loss: 0.0465, Test Loss: 0.0523\n",
      "Epoch [28/40], Train Loss: 0.0462, Test Loss: 0.0525\n",
      "Epoch [29/40], Train Loss: 0.0460, Test Loss: 0.0526\n",
      "Epoch [30/40], Train Loss: 0.0456, Test Loss: 0.0527\n",
      "Epoch [31/40], Train Loss: 0.0453, Test Loss: 0.0529\n",
      "Epoch [32/40], Train Loss: 0.0450, Test Loss: 0.0532\n",
      "Epoch [33/40], Train Loss: 0.0448, Test Loss: 0.0537\n",
      "Epoch [34/40], Train Loss: 0.0445, Test Loss: 0.0542\n",
      "Epoch [35/40], Train Loss: 0.0443, Test Loss: 0.0545\n",
      "Epoch [36/40], Train Loss: 0.0441, Test Loss: 0.0550\n",
      "Epoch [37/40], Train Loss: 0.0438, Test Loss: 0.0557\n",
      "Epoch [38/40], Train Loss: 0.0436, Test Loss: 0.0555\n",
      "Epoch [39/40], Train Loss: 0.0434, Test Loss: 0.0563\n",
      "Epoch [40/40], Train Loss: 0.0432, Test Loss: 0.0562\n",
      "[[ 0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.21  0.29  0.12 -0.13  1.26  0.    0.  ]\n",
      " [ 0.6   0.29 -0.08  1.43 -0.77  0.    0.  ]\n",
      " [ 0.26  0.58  1.4  -0.34 -0.63  0.    0.  ]\n",
      " [-0.65  1.08 -0.19  0.25  0.27  0.    0.  ]\n",
      " [ 1.65  0.26  0.51  0.26  0.69  0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "class PalindromeNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PalindromeNetwork2, self).__init__()\n",
    "        self.tokenizer = CustomTokenizer() \n",
    "        self.IPrime = nn.Parameter(torch.zeros(self.tokenizer.max_word_length,self.tokenizer.max_word_length))\n",
    "        self.fc2 = nn.Linear(26,26)\n",
    "        self.fc3 = nn.Linear(26,1)\n",
    "\n",
    "\n",
    "    def forward(self, P):\n",
    "        word_len = P.shape[0]\n",
    "        d = self.IPrime[-1*word_len:, :word_len].matmul(P)\n",
    "        out = P + d\n",
    "        out = self.fc2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sum(out, dim=0)\n",
    "        out = nn.functional.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "dataset = generate_dataset(10000, [3,4,5])\n",
    "tokenizer = CustomTokenizer()\n",
    "X = []\n",
    "for row in dataset:\n",
    "    X.append([tokenizer.to_matrix(row[0]), row[1]])\n",
    "\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_data, test_data = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training loop\n",
    "model = PalindromeNetwork2()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_data:\n",
    "        inputs, label = data\n",
    "        label = torch.tensor([label], dtype=torch.float32)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_data:\n",
    "            inputs, label = data\n",
    "            label = torch.tensor([label], dtype=torch.float32)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, label)\n",
    "            total_test_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_train_loss/len(train_data):.4f}, Test Loss: {total_test_loss/len(test_data):.4f}')\n",
    "\n",
    "print(model.IPrime.detach().numpy().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x313da73d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.IPrime.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Adding Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Train Loss: 0.7048, Test Loss: 0.6952\n",
      "Epoch [2/40], Train Loss: 0.4604, Test Loss: 0.1395\n",
      "Epoch [3/40], Train Loss: 0.0972, Test Loss: 0.0831\n",
      "Epoch [4/40], Train Loss: 0.0684, Test Loss: 0.0703\n",
      "Epoch [5/40], Train Loss: 0.0587, Test Loss: 0.0646\n",
      "Epoch [6/40], Train Loss: 0.0537, Test Loss: 0.0617\n",
      "Epoch [7/40], Train Loss: 0.0508, Test Loss: 0.0601\n",
      "Epoch [8/40], Train Loss: 0.0489, Test Loss: 0.0592\n",
      "Epoch [9/40], Train Loss: 0.0475, Test Loss: 0.0585\n",
      "Epoch [10/40], Train Loss: 0.0466, Test Loss: 0.0581\n",
      "Epoch [11/40], Train Loss: 0.0458, Test Loss: 0.0580\n",
      "Epoch [12/40], Train Loss: 0.0453, Test Loss: 0.0579\n",
      "Epoch [13/40], Train Loss: 0.0449, Test Loss: 0.0579\n",
      "Epoch [14/40], Train Loss: 0.0445, Test Loss: 0.0579\n",
      "Epoch [15/40], Train Loss: 0.0442, Test Loss: 0.0580\n",
      "Epoch [16/40], Train Loss: 0.0440, Test Loss: 0.0581\n",
      "Epoch [17/40], Train Loss: 0.0438, Test Loss: 0.0581\n",
      "Epoch [18/40], Train Loss: 0.0435, Test Loss: 0.0582\n",
      "Epoch [19/40], Train Loss: 0.0434, Test Loss: 0.0582\n",
      "Epoch [20/40], Train Loss: 0.0432, Test Loss: 0.0582\n",
      "Epoch [21/40], Train Loss: 0.0430, Test Loss: 0.0584\n",
      "Epoch [22/40], Train Loss: 0.0429, Test Loss: 0.0583\n",
      "Epoch [23/40], Train Loss: 0.0427, Test Loss: 0.0585\n",
      "Epoch [24/40], Train Loss: 0.0426, Test Loss: 0.0584\n",
      "Epoch [25/40], Train Loss: 0.0425, Test Loss: 0.0584\n",
      "Epoch [26/40], Train Loss: 0.0424, Test Loss: 0.0585\n",
      "Epoch [27/40], Train Loss: 0.0423, Test Loss: 0.0585\n",
      "Epoch [28/40], Train Loss: 0.0421, Test Loss: 0.0585\n",
      "Epoch [29/40], Train Loss: 0.0420, Test Loss: 0.0585\n",
      "Epoch [30/40], Train Loss: 0.0419, Test Loss: 0.0585\n",
      "Epoch [31/40], Train Loss: 0.0419, Test Loss: 0.0585\n",
      "Epoch [32/40], Train Loss: 0.0418, Test Loss: 0.0585\n",
      "Epoch [33/40], Train Loss: 0.0417, Test Loss: 0.0584\n",
      "Epoch [34/40], Train Loss: 0.0416, Test Loss: 0.0585\n",
      "Epoch [35/40], Train Loss: 0.0415, Test Loss: 0.0585\n",
      "Epoch [36/40], Train Loss: 0.0415, Test Loss: 0.0584\n",
      "Epoch [37/40], Train Loss: 0.0414, Test Loss: 0.0584\n",
      "Epoch [38/40], Train Loss: 0.0413, Test Loss: 0.0583\n",
      "Epoch [39/40], Train Loss: 0.0413, Test Loss: 0.0583\n",
      "Epoch [40/40], Train Loss: 0.0412, Test Loss: 0.0583\n",
      "[[ 0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.01  0.01  0.02  0.01  1.02  0.    0.  ]\n",
      " [ 0.05  0.02  0.    1.07 -0.02  0.    0.  ]\n",
      " [-0.    0.05  1.02 -0.03  0.02  0.    0.  ]\n",
      " [ 0.01  0.97  0.06  0.01 -0.01  0.    0.  ]\n",
      " [ 1.04  0.01  0.01  0.07  0.04  0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "class PalindromeNetwork3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PalindromeNetwork3, self).__init__()\n",
    "        self.tokenizer = CustomTokenizer() \n",
    "        self.IPrime = nn.Parameter(torch.zeros(self.tokenizer.max_word_length,self.tokenizer.max_word_length))\n",
    "        self.fc2 = nn.Linear(26,26)\n",
    "        self.fc3 = nn.Linear(26,1)\n",
    "\n",
    "\n",
    "    def forward(self, P):\n",
    "        word_len = P.shape[0]\n",
    "        d = self.IPrime[-1*word_len:, :word_len].matmul(P)\n",
    "        out = P + d\n",
    "        out = self.fc2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sum(out, dim=0)\n",
    "        out = nn.functional.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "dataset = generate_dataset(10000, [3,4,5])\n",
    "tokenizer = CustomTokenizer()\n",
    "X = []\n",
    "for row in dataset:\n",
    "    X.append([tokenizer.to_matrix(row[0]), row[1]])\n",
    "\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_data, test_data = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training loop\n",
    "model = PalindromeNetwork3()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_data:\n",
    "        inputs, label = data\n",
    "        label = torch.tensor([label], dtype=torch.float32)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Add regularization to encourage outputs closer to 0 or 1\n",
    "        regularization = 0.01 * torch.sum((model.IPrime * (1 - model.IPrime))**2)\n",
    "        loss += regularization\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_data:\n",
    "            inputs, label = data\n",
    "            label = torch.tensor([label], dtype=torch.float32)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, label)\n",
    "            total_test_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_train_loss/len(train_data):.4f}, Test Loss: {total_test_loss/len(test_data):.4f}')\n",
    "\n",
    "print(model.IPrime.detach().numpy().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x316cc9c10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.IPrime.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Network with Program Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Train Loss: 0.5691, Test Loss: 0.1865\n",
      "Epoch [2/40], Train Loss: 0.1114, Test Loss: 0.0846\n",
      "Epoch [3/40], Train Loss: 0.0690, Test Loss: 0.0673\n",
      "Epoch [4/40], Train Loss: 0.0571, Test Loss: 0.0599\n",
      "Epoch [5/40], Train Loss: 0.0516, Test Loss: 0.0564\n",
      "Epoch [6/40], Train Loss: 0.0484, Test Loss: 0.0548\n",
      "Epoch [7/40], Train Loss: 0.0465, Test Loss: 0.0540\n",
      "Epoch [8/40], Train Loss: 0.0451, Test Loss: 0.0538\n",
      "Epoch [9/40], Train Loss: 0.0442, Test Loss: 0.0537\n",
      "Epoch [10/40], Train Loss: 0.0435, Test Loss: 0.0537\n",
      "Epoch [11/40], Train Loss: 0.0429, Test Loss: 0.0540\n",
      "Epoch [12/40], Train Loss: 0.0425, Test Loss: 0.0542\n",
      "Epoch [13/40], Train Loss: 0.0421, Test Loss: 0.0546\n",
      "Epoch [14/40], Train Loss: 0.0418, Test Loss: 0.0547\n",
      "Epoch [15/40], Train Loss: 0.0416, Test Loss: 0.0550\n",
      "Epoch [16/40], Train Loss: 0.0413, Test Loss: 0.0551\n",
      "Epoch [17/40], Train Loss: 0.0411, Test Loss: 0.0554\n",
      "Epoch [18/40], Train Loss: 0.0410, Test Loss: 0.0557\n",
      "Epoch [19/40], Train Loss: 0.0408, Test Loss: 0.0559\n",
      "Epoch [20/40], Train Loss: 0.0407, Test Loss: 0.0561\n",
      "Epoch [21/40], Train Loss: 0.0405, Test Loss: 0.0563\n",
      "Epoch [22/40], Train Loss: 0.0404, Test Loss: 0.0565\n",
      "Epoch [23/40], Train Loss: 0.0403, Test Loss: 0.0566\n",
      "Epoch [24/40], Train Loss: 0.0402, Test Loss: 0.0568\n",
      "Epoch [25/40], Train Loss: 0.0401, Test Loss: 0.0569\n",
      "Epoch [26/40], Train Loss: 0.0400, Test Loss: 0.0570\n",
      "Epoch [27/40], Train Loss: 0.0399, Test Loss: 0.0571\n",
      "Epoch [28/40], Train Loss: 0.0398, Test Loss: 0.0572\n",
      "Epoch [29/40], Train Loss: 0.0397, Test Loss: 0.0573\n",
      "Epoch [30/40], Train Loss: 0.0397, Test Loss: 0.0574\n",
      "Epoch [31/40], Train Loss: 0.0396, Test Loss: 0.0575\n",
      "Epoch [32/40], Train Loss: 0.0395, Test Loss: 0.0576\n",
      "Epoch [33/40], Train Loss: 0.0394, Test Loss: 0.0577\n",
      "Epoch [34/40], Train Loss: 0.0393, Test Loss: 0.0578\n",
      "Epoch [35/40], Train Loss: 0.0393, Test Loss: 0.0578\n",
      "Epoch [36/40], Train Loss: 0.0392, Test Loss: 0.0578\n",
      "Epoch [37/40], Train Loss: 0.0391, Test Loss: 0.0579\n",
      "Epoch [38/40], Train Loss: 0.0391, Test Loss: 0.0579\n",
      "Epoch [39/40], Train Loss: 0.0390, Test Loss: 0.0580\n",
      "Epoch [40/40], Train Loss: 0.0390, Test Loss: 0.0580\n"
     ]
    }
   ],
   "source": [
    "class PalindromeNetwork4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PalindromeNetwork4, self).__init__()\n",
    "        self.tokenizer = CustomTokenizer() \n",
    "        self.IPrime = nn.Parameter(torch.zeros(self.tokenizer.max_word_length,self.tokenizer.max_word_length))\n",
    "        self.fc2 = nn.Linear(26,26)\n",
    "        self.fc3 = nn.Linear(26,1)\n",
    "\n",
    "\n",
    "    def forward(self, P):\n",
    "        word_len = P.shape[0]\n",
    "        d = self.IPrime[-1*word_len:, :word_len].matmul(P)\n",
    "        out = P + d\n",
    "        out = self.fc2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sum(out, dim=0)\n",
    "        out = nn.functional.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "dataset = generate_dataset(10000, [3,4,5])\n",
    "tokenizer = CustomTokenizer()\n",
    "X = []\n",
    "for row in dataset:\n",
    "    X.append([tokenizer.to_matrix(row[0]), row[1]])\n",
    "\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_data, test_data = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training loop\n",
    "model = PalindromeNetwork4()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_data:\n",
    "        inputs, label = data\n",
    "        label = torch.tensor([label], dtype=torch.float32)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Add regularization to encourage outputs closer to 0 or 1\n",
    "        regularization = 0.01 * torch.sum((model.IPrime * (1 - model.IPrime))**2)\n",
    "        loss += regularization\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_data:\n",
    "            inputs, label = data\n",
    "            label = torch.tensor([label], dtype=torch.float32)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, label)\n",
    "            total_test_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_train_loss/len(train_data):.4f}, Test Loss: {total_test_loss/len(test_data):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found condition: reverse_equality_condition\n",
      "tensor([0.9978], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9853], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def synthesize_rule(target_matrix, in_distribution_indices):\n",
    "    def equality_condition(a, b):\n",
    "        return a == b\n",
    "    def reverse_equality_condition(a, b):\n",
    "        return target_matrix.shape[0] - a == b\n",
    "    def inequality_condition(a, b):\n",
    "        return a != b\n",
    "    def greater_than_condition(a, b):\n",
    "        return a > b\n",
    "    def less_than_condition(a, b):\n",
    "        return a < b\n",
    "    # ... continue with a comprehensive DSL of conditions\n",
    "\n",
    "    possible_conditions = [\n",
    "        equality_condition,\n",
    "        reverse_equality_condition,\n",
    "        inequality_condition,\n",
    "        greater_than_condition,\n",
    "        less_than_condition,\n",
    "        # ... add more conditions as needed\n",
    "    ]\n",
    "\n",
    "    # Perform a very basic search for a condition that holds for all elements\n",
    "    # This is a placeholder for a more sophisticated search algorithm\n",
    "    for condition in possible_conditions:\n",
    "        condition_holds = True\n",
    "        for i in range(target_matrix.shape[0]):\n",
    "            for j in range(target_matrix.shape[1]):\n",
    "\n",
    "                # Skip elements not in the in-distribution indices - they will not contain\n",
    "                # any information about the rule\n",
    "                if i not in in_distribution_indices or j not in in_distribution_indices:\n",
    "                    continue\n",
    "\n",
    "                # Check if the condition holds for all elements that are in-distribution\n",
    "                if target_matrix[i][j] == 1 and not condition(i, j):\n",
    "                    condition_holds = False\n",
    "                    break\n",
    "                if target_matrix[i][j] == 0 and condition(i, j):\n",
    "                    condition_holds = False\n",
    "                    break\n",
    "        if condition_holds:\n",
    "            print(f\"Found condition: {condition.__name__}\")\n",
    "            return condition\n",
    "    return None\n",
    "\n",
    "\n",
    "# Get I'\n",
    "discretized_matrix = model.IPrime.detach().numpy()\n",
    "\n",
    "# Round the values to 0 or 1\n",
    "discretized_matrix[discretized_matrix < 0.5] = 0\n",
    "discretized_matrix[discretized_matrix >= 0.5] = 1\n",
    "\n",
    "condition_rule = synthesize_rule(model.IPrime.detach().numpy(), [3,4,5])\n",
    "with torch.no_grad():\n",
    "    for i in range(model.IPrime.shape[0]):\n",
    "        for j in range(model.IPrime.shape[1]):\n",
    "            if condition_rule(i, j):\n",
    "                model.IPrime[i][j] = 1\n",
    "            else:\n",
    "                model.IPrime[i][j] = 0\n",
    "\n",
    "\n",
    "print(model(tokenizer.to_matrix(\"abbba\")))\n",
    "print(model(tokenizer.to_matrix(\"abbbba\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
