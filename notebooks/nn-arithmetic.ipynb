{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        # Define the special tokens\n",
    "        self.special_tokens = [\"Input:\", \"Target:\", \"<scratch>\", \"</scratch>\", \"A->\", \"C->\", ',', ' ', '\\n', '+','1','2','3','4','5','6','7','8','9','0', '<PAD>', '<START>', '<END>']\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.special_tokens)}\n",
    "        self.id_to_token = {i: token for token, i in self.token_to_id.items()}\n",
    "        self.vocab = set(self.special_tokens)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.max_length = 50\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text into a list of tokens, properly handling newlines and special tokens.\n",
    "        \"\"\"\n",
    "        # First preserve newlines by replacing them with a special placeholder\n",
    "        text = text.replace('\\n', ' <NEWLINE> ')\n",
    "        \n",
    "        # Handle other special tokens\n",
    "        for token in self.special_tokens:\n",
    "            text = text.replace(token, f\" {token} \")\n",
    "        \n",
    "        # Split on whitespace and filter out empty strings\n",
    "        tokens = [token for token in text.split() if token]\n",
    "        \n",
    "        # Replace newline placeholder back if needed\n",
    "        tokens = ['\\n' if t == '<NEWLINE>' else t for t in tokens]\n",
    "        \n",
    "        # Update vocabulary\n",
    "        self.vocab.update(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    " \n",
    "        \n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        Encode a list of tokens into their corresponding IDs.\n",
    "        \"\"\"\n",
    "        return [self.token_to_id.get(token, len(self.token_to_id)) for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a text string.\n",
    "        \"\"\"\n",
    "        tokens = [self.id_to_token.get(token_id, \"<unk>\") for token_id in token_ids]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def pad(self, data_tokenized):\n",
    "        \"\"\"\n",
    "        Pad a list of tokenized data so that all sequences have the same length.\n",
    "        \"\"\"\n",
    "        row = data_tokenized + [self.token_to_id['<PAD>']] * (self.max_length - len(data_tokenized))\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: torch.Size([32000, 50])\n",
      "Validation data: torch.Size([8000, 50])\n"
     ]
    }
   ],
   "source": [
    "def equation_to_prompt(a,b, with_answer=True):\n",
    "    \"\"\"\n",
    "    Convert an equation string into a prompt string.\n",
    "    \"\"\"\n",
    "    input_str = f\"{a}+{b}\"\n",
    "    scratch = \"<scratch>\\n\"\n",
    "    carry = 0\n",
    "    for i, (digit_a, digit_b) in enumerate(zip(str(a)[::-1], str(b)[::-1])):\n",
    "        sum_digits = int(digit_a) + int(digit_b) + carry\n",
    "        carry = sum_digits // 10\n",
    "        scratch += f\"A->{sum_digits % 10}, C->{carry}\\n\"\n",
    "    \n",
    "    # Append remaining digits of the longer number\n",
    "    remaining = str(a)[::-1][len(str(b)):] if len(str(a)) > len(str(b)) else str(b)[::-1][len(str(a)):]\n",
    "\n",
    "    for digit in remaining:\n",
    "        sum_digits = int(digit) + carry\n",
    "        carry = sum_digits // 10\n",
    "        scratch += f\"A->{sum_digits % 10}, C->{carry}\\n\"\n",
    "    \n",
    "    if carry > 0:\n",
    "        scratch += f\"A->{carry}, C->0\\n\"\n",
    "    \n",
    "    scratch += \"</scratch>\"\n",
    "    \n",
    "    # Combine into the target\n",
    "    target_str = f\"{scratch}\\n{a + b}<END>\" if with_answer else \"<scratch>\\n\"\n",
    "    \n",
    "    return f\"<START>Input:\\n{input_str}\\n\\nTarget:\\n{target_str}\"\n",
    "\n",
    "def generate_training_data(limit=1000):\n",
    "    dataset = []\n",
    "    for a in range(limit):\n",
    "        for b in range(limit):\n",
    "            example = equation_to_prompt(a, b)\n",
    "            example_tokenized = tokenizer.encode(tokenizer.tokenize(example))\n",
    "            example_padded = tokenizer.pad(example_tokenized)\n",
    "            dataset.append(example_padded)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Generate the dataset\n",
    "tokenizer = CustomTokenizer()\n",
    "data = generate_training_data(limit=200)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, shuffle=True, random_state=RANDOM_SEED)\n",
    "train_data, val_data = torch.tensor(train_data), torch.tensor(val_data)\n",
    "print(f\"Training data: {train_data.shape}\")\n",
    "print(f\"Validation data: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [1,0,0,0]\n",
    "])\n",
    "rev = np.array([\n",
    "    [0, 0,1],\n",
    "    [0, 1,0],\n",
    "    [1,0,0,]\n",
    "])\n",
    "# flip the rows of x\n",
    "np.dot(rev,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# POSITIONAL EMBEDDING\n",
    "############################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len):\n",
    "        super().__init__()\n",
    "        # We'll just use a learnable embedding [max_len, embed_dim].\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, embed_dim]\n",
    "        We'll add a positional embedding for each position in the sequence.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # positions = [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        # pos_embed[positions] -> [1, seq_len, embed_dim]\n",
    "        return x + self.pos_embed(positions)\n",
    "    \n",
    "\n",
    "###################################\n",
    "# MASKED MULTI-HEAD SELF ATTENTION\n",
    "###################################\n",
    "def causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a lower-triangular mask for causal attention of shape [seq_len, seq_len].\n",
    "    Entry (i, j) is True if position j is masked out for position i (j > i).\n",
    "    We'll convert it to a float mask for use in attention.\n",
    "    \"\"\"\n",
    "    # mask[i, j] = True if j > i, i.e. future positions\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Query, Key, Value projection layers\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x)  # [B, T, C]\n",
    "        k = self.k_proj(x)  # [B, T, C]\n",
    "        v = self.v_proj(x)  # [B, T, C]\n",
    "\n",
    "        # Reshape to [B, num_heads, T, head_dim]\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # [B, nh, T, hd]\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # [B, nh, T, hd]\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # [B, nh, T, hd]\n",
    "\n",
    "        # Compute attention scores: [B, nh, T, T]\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Causal mask\n",
    "        causal = causal_mask(T).to(x.device)  # [T, T]\n",
    "        attn_scores = attn_scores.masked_fill(causal, float('-inf'))\n",
    "\n",
    "        # Softmax\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)  # [B, nh, T, T]\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = torch.matmul(attn_probs, v)  # [B, nh, T, hd]\n",
    "\n",
    "        # Recombine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # [B, T, embed_dim]\n",
    "\n",
    "        # Output projection\n",
    "        out = self.out_proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "############################\n",
    "# TRANSFORMER DECODER BLOCK\n",
    "############################\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, embed_dim]\n",
    "\n",
    "        # Self-attention\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out = self.attn(x_norm)\n",
    "        x = x + attn_out  # residual\n",
    "\n",
    "        # Feed-forward\n",
    "        x_norm = self.ln2(x)\n",
    "        ffn_out = self.ffn(x_norm)\n",
    "        x = x + ffn_out   # residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "############################\n",
    "# GPT\n",
    "############################\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = PositionalEncoding(embed_dim, max_seq_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(embed_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)  # final layer norm\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        idx: [B, T] of token indices\n",
    "        Returns: logits [B, T, vocab_size]\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # 1. Embed tokens\n",
    "        x = self.token_emb(idx)  # [B, T, embed_dim]\n",
    "\n",
    "        # 2. Add positional embeddings\n",
    "        x = self.pos_emb(x)      # [B, T, embed_dim]\n",
    "\n",
    "        # 3. Pass through decoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # 4. Final layer norm\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # 5. Output head\n",
    "        logits = self.head(x)    # [B, T, vocab_size]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH 0/1000,  LOSS: 3.2178728580474854\n",
      "BATCH 100/1000,  LOSS: 0.896584153175354\n",
      "BATCH 200/1000,  LOSS: 0.6238868236541748\n",
      "BATCH 300/1000,  LOSS: 0.5347651839256287\n",
      "BATCH 400/1000,  LOSS: 0.5086193084716797\n",
      "BATCH 500/1000,  LOSS: 0.4683353006839752\n",
      "BATCH 600/1000,  LOSS: 0.46341222524642944\n",
      "BATCH 700/1000,  LOSS: 0.43469005823135376\n",
      "BATCH 800/1000,  LOSS: 0.40084734559059143\n",
      "BATCH 900/1000,  LOSS: 0.35798534750938416\n",
      "Epoch 1 | Train Loss: 0.5907 | Val Loss: 0.3298\n",
      "BATCH 0/1000,  LOSS: 0.3355371654033661\n",
      "BATCH 100/1000,  LOSS: 0.33056873083114624\n",
      "BATCH 200/1000,  LOSS: 0.3253413438796997\n",
      "BATCH 300/1000,  LOSS: 0.3217582404613495\n",
      "BATCH 400/1000,  LOSS: 0.319132000207901\n",
      "BATCH 500/1000,  LOSS: 0.32011303305625916\n",
      "BATCH 600/1000,  LOSS: 0.32148075103759766\n",
      "BATCH 700/1000,  LOSS: 0.31413790583610535\n",
      "BATCH 800/1000,  LOSS: 0.3162676692008972\n",
      "BATCH 900/1000,  LOSS: 0.3226606845855713\n",
      "Epoch 2 | Train Loss: 0.3227 | Val Loss: 0.3132\n"
     ]
    }
   ],
   "source": [
    "# Example hyperparameters\n",
    "vocab_size = tokenizer.vocab_size     # size of your vocabulary\n",
    "embed_dim = 128                             # embedding dimension\n",
    "num_heads = 4                               # number of attention heads\n",
    "num_layers = 4                              # number of decoder layers\n",
    "sequence_length = 50                        # max sequence length in  dataset\n",
    "dropout = 0.1                               # dropout rate\n",
    "batch_size = 32\n",
    "num_batches = len(train_data) // batch_size\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_data)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    max_seq_len=sequence_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for batch in data_loader:\n",
    "        (x,) = batch\n",
    "        x = x.to(device)\n",
    "\n",
    "        # GPT input: x[:, :-1], target: x[:, 1:]\n",
    "        input_tokens = x[:, :-1]\n",
    "        target_tokens = x[:, 1:]\n",
    "\n",
    "        logits = model(input_tokens)  # [B, T-1, vocab_size]\n",
    "        B, Tm1, V = logits.shape\n",
    "\n",
    "        # Make tensors contiguous or use .reshape\n",
    "        logits_2d = logits.contiguous().view(B * Tm1, V)\n",
    "        targets_1d = target_tokens.contiguous().view(B * Tm1)\n",
    "\n",
    "        # Cross entropy\n",
    "        loss = F.cross_entropy(logits_2d, targets_1d)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"BATCH {i}/{num_batches},  LOSS: {loss.item()}\")\n",
    "\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        (x,) = batch\n",
    "        x = x.to(device)\n",
    "\n",
    "        input_tokens = x[:, :-1]\n",
    "        target_tokens = x[:, 1:]\n",
    "        logits = model(input_tokens)\n",
    "\n",
    "        B, Tm1, V = logits.shape\n",
    "\n",
    "        logits_2d = logits.contiguous().view(B * Tm1, V)\n",
    "        targets_1d = target_tokens.contiguous().view(B * Tm1)\n",
    "        loss = F.cross_entropy(logits_2d, targets_1d)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, num1, num2, max_length=50):\n",
    "    # Build the prompt\n",
    "    prompt = equation_to_prompt(num1, num2, with_answer=False)\n",
    "    # Tokenize + encode\n",
    "    tokens = tokenizer.tokenize(prompt)\n",
    "    start_tokens = tokenizer.encode(tokens)\n",
    "\n",
    "    model.eval()\n",
    "    while len(start_tokens) < max_length:\n",
    "        # Convert current tokens into a tensor; no padding, just the raw sequence\n",
    "        input_ids = torch.tensor([start_tokens], dtype=torch.long).to(device)\n",
    "        logits = model(input_ids)  # [1, current_length, vocab_size]\n",
    "        \n",
    "        # Take the logits from the last time-step\n",
    "        next_token_logits = logits[0, -1, :]  # [vocab_size]\n",
    "        next_token_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "        # If the model predicts <PAD> or <END>, we can choose to stop\n",
    "        # for example, if next_token_id == tokenizer.token_to_id['<PAD>']:\n",
    "        #     break\n",
    "\n",
    "        # Append next token to our running sequence\n",
    "        start_tokens.append(next_token_id)\n",
    "    \n",
    "    # Decode the final token list\n",
    "    answer = tokenizer.decode(start_tokens)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> Input: \\n 2 + 5 \\n \\n Target: \\n <scratch> \\n A-> 5 , C-> 0 \\n </scratch> \\n 5 <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, 2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
