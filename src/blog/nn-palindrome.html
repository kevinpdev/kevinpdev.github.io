<!--TEMPLATE:blog.html-->

<h1>Learning Palindromes with Neural Networks & Program Synthesis</h1>
<p>
    The palindrome problem is a classic introductory problem in computer science. 
    It involves determining whether a given string is the same when read forwards and backwards.
    For example, the string "racecar" is a palindrome because it reads the same forwards and backwards.
    The string "hello" is not a palindrome because it reads differently forwards and backwards.
</p>

<pre><code class="language-python">def is_palindrome(s):
    return s == s[::-1]</code></pre>

<p>
    Traditionally, transformers and deep learning architectures in general have struggled with algorithmic thinking [1].
    One hypothesis is that they memorize solutions to problems rather than learning the underlying algorithm [2]. It is
    interesting to see how well LLMs can solve certain problems, but can struggle with algorithmic problems. A quick
    test of the palindrome problem on GPT 3.5, GPT 4o, and Gemini 1.5 Flash shows that they all can fail to solve the problem
    if the palindrome is uncommon (unlikely to be found in the training data).
</p>

<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-1.png" alt="GPT 3.5 Palindrome Prompt" style="width: 80%">
    <small>GPT 3.5 Palindrome Prompt - Incorrect Answer</small>
</div>
<br/>
<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-2.png" alt="GPT 4o Palindrome Prompt" style="width: 80%">
    <small>GPT 4o Palindrome Prompt - Incorrect Answer</small>
</div>
<br/>
<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-3.png" alt="Gemini 1.5 Flash Palindrome Prompt" style="width: 80%">
    <small>Gemini 1.5 Flash Prompt - Incorrect Answer</small>
</div>
<br/>

<h2>Why?</h2>
<p>
    It is curious as to why LLMs and deep learning models in general are particularly bad in algorithmic thinking. This
    article explores very simple algorithmic problems and neural network architectures
    to see if we can get a neural network to learn the algorithm.
</p>



<h2>Creating a Differentiable Palindrome Architecture From Scratch</h2>
<p>
    One interesting exercise would be to see if we could create a deep learning model that can solve the palindrome problem
    from scratch (without training, manually initializing weights). Then we could at least be confident that there is a network
    that could theoretically solve the problem. 
</p>

<p>
    The actual network to do this is actually rather straightforward. We can do the equivalent of this algorithm in a
    neural network computation graph:
        
</p>

<ol>
    <li>1. Create a reversed copy of given word.</li>
    <li>2. Check if the reversed copy is equal to the original word.</li>
</ol>
<p>In a neural network using vectors, that might look like the following:</p>
<ol>
    <li>1. One hot encode the characters of the original word (into a matrix)</li>
    <li>2. Create a reversed copy of those one hot encoded vectors (transpose the matrix)</li>
    <li>3. Perform matrix multiplication and aggregate</li>
</ol>
    
<p>
    Let's walk through a small example to illustrate this, for the sake of simplicity we will pretend our vocabulary
    is only "a,b,c,d" so that we can easily visualize the one hot encoding. We will use the word "aba" as our example.
</p>
<p>
    First, we one hot encode the word "aba" into a matrix.
    This is a 3x4 matrix, where each row represents a character in the word "aba".
</p>
<div style="text-align: center;">
    <math-field style="font-size: 26px;" readonly>P = onehot("aba") = \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0 \\
        \end{bmatrix}
    </math-field>
</div>  
<p>
    Then we can reverse the rows of the matrix to get the reversed copy of the word "aba".
    To do this we can use the reverse identity matrix I' which is a square matrix of size mxm
    where m is the length of the word.
</p>
<div style="text-align: center;">
    <math-field style="font-size: 26px;" readonly>P' = rev(onehot("aba")) = I'P = \begin{bmatrix}
        0 & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & 0  \\
        \end{bmatrix}
        \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0 \\
        \end{bmatrix}
    = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    \end{bmatrix}
    </math-field>
</div>  
<p>
    Finally, we can perform a matrix multiplication of the original matrix and the reversed matrix.
    This will give us a matrix where each row is the dot product of the original and reversed vectors.
    We can then need to find if these matrices are equal in a differentiable way. We can do this by 
    taking the difference of the two matrices, and then calculating the frobius norm. This will give us the 
    total difference between the word and the reversed word. We will call this d
</p>
<div style="text-align: center;">
    <math-field style="font-size: 26px;" readonly>d = \sqrt{\sum_{i,j}(P-I'P)_{i,j}^2} = 
        \sqrt{\sum_{i,j}
        \lparen
        \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0 \\
        \end{bmatrix}
        - \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0 \\
        \end{bmatrix}\rparen_{i,j}^2}
    </math-field>
    <math-field style="font-size: 26px;" readonly>d = \sqrt{\sum_{i,j}(P-I'P)_{i,j}^2} = 
        \sqrt{\sum_{i,j}
        \begin{bmatrix}
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        \end{bmatrix}_{i,j}^2}
    </math-field>
    <br/>
    <math-field style="font-size: 26px;" readonly>d = 0
    </math-field>
</div>  
<p>
    Becuase d=0, our word is a palindrome. We can repeat the process with a non-palindrome and
    see that d will be greater than 0.
</p>
<div style="text-align: center;">
    <math-field style="font-size: 26px;" readonly>P = onehot("abb") = \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        \end{bmatrix}
    </math-field>
    <br/>
    <math-field style="font-size: 26px;" readonly>P' = rev(onehot("abb")) = I'P = \begin{bmatrix}
        0 & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & 0  \\
        \end{bmatrix}
        \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        \end{bmatrix}
    = \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    \end{bmatrix}
    </math-field>
    <br/>
    <math-field style="font-size: 26px;" readonly>d = \sqrt{\sum_{i,j}(P-I'P)_{i,j}^2} = 
        \sqrt{\sum_{i,j}
        \lparen
        \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        \end{bmatrix}
        - \begin{bmatrix}
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0 \\
        \end{bmatrix}\rparen_{i,j}^2}
    </math-field>
    <math-field style="font-size: 26px;" readonly>d = \sqrt{\sum_{i,j}(P-I'P)_{i,j}^2} = 
        \sqrt{\sum_{i,j}
        \begin{bmatrix}
        1 & -1 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        -1 & 1 & 0 & 0 \\
        \end{bmatrix}_{i,j}^2}
    </math-field>
    <br/>
    <math-field style="font-size: 26px;" readonly>d = \sqrt{1^2 + (-1)^2 + 1^2 + (-1)^2} = 2
</div> 
<br/>
<p>Based on this example we can see that when d > 0, then the word is not a palindrome. When d=0, the word is a palindrome.</p>

<br/>
<h2>Putting the Model Together</h2>
<p>Now that we have gone through some examples, lets write out the full palindrome model in math, and then codeify it.</p>
<math-field style="font-size: 26px;" readonly>
    \[
    \text{prediction} = 
    \begin{cases} 
    \text{"not palindrome"} & \text{if } \sqrt{\sum_{i,j}(P-I'P)_{i,j}^2} > 0 \\
    \text{"palindrome"} & \text{if } \sqrt{\sum_{i,j}(P-I'P)_{i,j}^2} \leq 0 
    \end{cases}
    \]
</math-field>
<br/>
Where P is the one hot encoded matrix of the input word, and I' is the reversed identity matrix.
<br/>
<br/>
<p>
    It is straightforward to model this in pytorch as a neural network. The code (optimized for readability,
    not performance) is as follows:
</p>

<pre><code class="language-python">class CustomTokenizer:
    def __init__(self):
        # Define the special tokens
        self.tokens = list(string.ascii_lowercase)
        self.token_to_id = {token: i for i, token in enumerate(self.tokens)}
        self.id_to_token = {i: token for token, i in enumerate(self.tokens)}
        self.vocab = set(self.tokens)
        self.vocab_size = len(self.vocab)
        self.max_word_length = 200
    
    def to_matrix(self, word):
        rows = len(word)
        cols = len(self.vocab)
        matrix = torch.zeros((rows, cols), dtype=torch.float32)
        for i, char in enumerate(word):
            matrix[i, self.token_to_id[char]] = 1.0
        return matrix
    
class PalindromeNetwork(nn.Module):
    def __init__(self):
        super(PalindromeNetwork, self).__init__()
        self.tokenizer = CustomTokenizer() 
        self.IPrime = torch.flip(torch.eye(self.tokenizer.max_word_length), [0])
        

    def forward(self, x):
        word_len = len(x)
        P = self.tokenizer.to_matrix(x)
        d = torch.norm(P - self.IPrime[-1*word_len:, :word_len].matmul(P), p="fro")
        return d

# Example usage
net = PalindromeNetwork()

print("Output for 'aba': ", net("aba").item())   # 0.0
print("Output for 'abb': ", net("abb").item())   # 2.0
print("Output for 'cba': ", net("cba").item())   # 2.0
print("Output for 'racecar': ", net("racecar").item())    # 0.0
print("Output for 'saippuakivikauppias': ", net("saippuakivikauppias").item())   # 0.0</code></pre>

<br/>
<p>Using our rule of thumb from above, we can see that the model can accurately 
    predict whether a word is a palindrome or not. It is able to 
    predict words up to arbitrary lengths (whatever we set the max length in the tokenizer to).
    It uses an accurate algorithm to solve the problem.
</p>
<br/>
<br/>

<h2>Learning the Algorithm</h2>
<p>
    In our demo model, there are a few key architectural decisions that we made that are highly biased towards being able
    to solve the palindrome problem in a generalized way. 
</p>
<ul>
    <li>The most obvious used a reverse identity matrix multiplication operation which tells the network that we need to perform a reverse operation.</li>
    <li>We also do a subtraction between the reversed matrix and the original matrix which biases the algorithm to know that we need to compare those two.</li>
    <li>We also take the norm of the final matrix, which biases the model to know that we are comparing want to measure the magnitude of the comparision.</li>
</ul>

All of these steps are allowing the network to solve the algorithm in a generalized way, but are
using "top down" algorithmic thinking that a human has to encode. We would like to get to a state
to where the network "learns" the algorithm itself. We can try to run some expieriments to see how well 
we can get the network to learn the algorithm itself.
<br/>
<h3>Expirement 1: Learning the Reverse Identity Matrix</h3>
<p> 
    We can change our neural network to have a learnable matrix instead of a reverse identity matrix
    with the following code: 
</p>
<pre><code class="language-python">class PalindromeNetwork(nn.Module):
    def __init__(self):
        super(PalindromeNetwork, self).__init__()
        self.tokenizer = CustomTokenizer() 
        self.IPrime = nn.Parameter(torch.zeros(self.tokenizer.max_word_length,self.tokenizer.max_word_length))

    def forward(self, P):
        word_len = P.shape[0]
        d = torch.norm(P - self.IPrime[-1*word_len:, :word_len].matmul(P), p="fro")
        d = nn.functional.tanh(d)
        return d</code></pre>
<br/>
<p>
    Now we can generate a palindrome dataset and run the training loop for this network.
    (the dataset generation code can be found on github). We will generate the dataset using 
    only palindromes of length 3,4,5, even though our network can handle words of lengths up to 7 ideally.
    This way we can see what if network learns to do the reverse operation.
</p>
<pre><code class="language-python">
    dataset = generate_dataset(10000, [3,4,5])
    tokenizer = CustomTokenizer()
    X = []
    for row in dataset:
        X.append([tokenizer.to_matrix(row[0]), row[1]])
    
    
    # Training loop
    model = PalindromeNetwork()
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    num_epochs = 7
    for epoch in range(num_epochs):
        total_loss = 0
        for data in X:
            inputs, label = data
            label = torch.tensor([label], dtype=torch.float32)
            
            # Zero the parameter gradients
            optimizer.zero_grad()
            
            # Forward pass
            outputs = model(inputs)
            
            loss = criterion(outputs, label)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(X):.4f}')</code></pre>
<br/>
<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-4.png" alt="Output of Training" style="width: 80%">
    <small>Training Output - Learning Reverse Identity</small>
</div>

<p>As shown in the screenshot above, the model succesfully learns something close enough to the reverse identity matrix.
    It is not perfect, but it is close enough to be able to solve the palindrome problem. 
    This shows that the network is able to learn the reverse operation, which is a key part of the algorithm.
</p>
<p>
    However, that is not the whole story. The matrix was only learned for lengths of the matrix that it was exposed to
    in the training data. It is not able to extrapolate to longer lengths. This means that if we try to give a word
    with a length outside of the training data, the model will not be able to solve it. This seems obvious, why would
    backprop would be need to extrapolate to longer lengths if it was not needed in the training data? There will never 
    be an error that has a gradient flow to those parameters due to the matrix cropping procedure that we use.
</p>
<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-5.png" alt="Output of Training" style="width: 80%">
    <small>Extrapolation Failure</small>
</div>
<br/>
<p>
    There is however a pattern in the learnable parameter matrix itself. If we completed the diagonal line
    and made it a perfect reverse identity matrix, we would be able to solve the problem for any length.
    Before thinking of ways to get the model to extrapolate, we can try to see if we debias the model even 
    more and still find patterns in the matrix like this.
</p>

<br/>
<br/>
<h3>Expirement 2: Generalizing the Norm</h3>
<p>
    Instead of manually taking the frobius norm, we would like to use fully connected layers and let the model figure that out,
    to make the model architecture more generalized.
</p>
<pre><code class="language-python">class PalindromeNetwork2(nn.Module):
    def __init__(self):
        super(PalindromeNetwork2, self).__init__()
        self.tokenizer = CustomTokenizer() 
        self.IPrime = nn.Parameter(torch.zeros(self.tokenizer.max_word_length,self.tokenizer.max_word_length))
        self.fc2 = nn.Linear(26,26)
        self.fc3 = nn.Linear(26,1)


    def forward(self, P):
        word_len = P.shape[0]
        d = self.IPrime[-1*word_len:, :word_len].matmul(P)
        out = P + d
        out = self.fc2(out)
        out = nn.functional.relu(out)
        out = self.fc3(out)
        out = nn.functional.sigmoid(out)
        return out</code></pre>
<p>
    Using this architecture, the model still solves the problem, The I' matrix still resembles a reverse
     identity matrix (with -1s) but it is "less clean" - harder to tell just by looking at it.
</p>

<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-6.png" alt="Output of Training" style="width: 80%">
    <small>Problem Solved - Without Regularization, Less Obvious Patterns in I'</small>
</div>

<br/>
<h3>Regularizing I'</h3>
<p>
    In order to attempt to get the model to learn a cleaner reverse identity matrix, we can try to regularize the I' matrix
    so that the entries are close to 1 or 0. We can do this by adding a regularization term to the loss function.
</p>
<p>We will use the following term to regularize the loss:</p>
<br/>

<div style="text-align: center;">
<math-field style="font-size: 26px;" readonly>
    \[
    \sum_{ij}(I' * (1 - I'))^2
    \]
</math-field>
</div>
<br/>
We do this so that the entries in I' are close to 0 or 1, which would make it more likely to be a reverse identity matrix.
This could also be interesting in a more generalized sense, to encourage the model to learn a binary (discretized) output.
<br/>
<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-8.png" alt="Regularization Function" style="width: 70%">
    <small>Regularization Function</small>
</div>
<br/>
<p>We can add it to the code with the following:</p>
<pre><code class="language-python">loss = criterion(outputs, label)

# Add regularization to encourage outputs closer to 0 or 1
regularization = 0.01 * torch.sum((model.IPrime * (1 - model.IPrime))**2)
loss += regularization</code></pre>

<br/>
<p>
    After training the model with this regularization, we can see that the model is able to solve the problem, and the I' matrix
    is more likely to be a reverse identity matrix.
</p>
<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-7.png" alt="Output of Training" style="width: 80%">
    <small>Problem Solved - With Regularization</small>
</div>

<br/>
<br/>
<h2>Extrapolating with Program Synthesis / Rule Mining </h2>
<p>
    One thing that we can do with a binary and discrete output is use a more traditional algorithm to extrapolate to new lengths.
    In this case, we are trying to "expand" the I' matrix to longer lengths, with the same pattern that we have learned with our
    training data.
</p>
<p>
    We could use discrete rule mining or program synthesis to come up with the following rule:
</p>
<pre><code class="language-python">I''[i][j] == 1 if i + j == width(I'') - 1 else I''[i][j] == 0</code></pre>
<br/>

<p>
    Here is a very basic program synthesis algorithm that is able to find that rule. In practice we would want a robust program 
    synthesis algorithm that searches for patterns in the matrix from the in-distribution rows/columns.
</p>

<p>
    We can then modify the intermediate parameter using this algorithm rather than backpropagating through the network.
</p>

<br/>
<p>After making these changes, we can see that our model is now able to extrapolate to new lengths that it has never seen before.</p>
<pre><code class="language-python">def synthesize_rule(target_matrix, in_distribution_indices):
    def equality_condition(a, b):
        return a == b
    def reverse_equality_condition(a, b):
        return a + b == target_matrix.shape[1] - 1
    def inequality_condition(a, b):
        return a != b
    def greater_than_condition(a, b):
        return a > b
    def less_than_condition(a, b):
        return a < b
    # ... continue with a comprehensive DSL of conditions

    possible_conditions = [
        equality_condition,
        reverse_equality_condition,
        inequality_condition,
        greater_than_condition,
        less_than_condition,
        # ... add more conditions as needed
    ]

    # Perform a very basic search for a condition that holds for all elements
    # This is a placeholder for a more sophisticated search algorithm
    for condition in possible_conditions:
        condition_holds = True
        for i in range(target_matrix.shape[0]):
            for j in range(target_matrix.shape[1]):

                # Skip elements not in the in-distribution indices - they will not contain
                # any information about the rule
                if i not in in_distribution_indices or j not in in_distribution_indices:
                    continue

                # Check if the condition holds for all elements that are in-distribution
                if target_matrix[i][j] == 1 and not condition(i, j):
                    condition_holds = False
                    break
                if target_matrix[i][j] == 0 and condition(i, j):
                    condition_holds = False
                    break
        if condition_holds:
            print(f"Found condition: {condition.__name__}")
            return condition
    return None


# Get I'
discretized_matrix = model.IPrime.detach().numpy()

# Round the values to 0 or 1
discretized_matrix[discretized_matrix < 0.5] = 0
discretized_matrix[discretized_matrix >= 0.5] = 1

condition_rule = synthesize_rule(model.IPrime.detach().numpy(), [3,4,5])
with torch.no_grad():
    for i in range(model.IPrime.shape[0]):
        for j in range(model.IPrime.shape[1]):
            if condition_rule(i, j):
                model.IPrime[i][j] = 1
            else:
                model.IPrime[i][j] = 0



print("Palindrome", model(tokenizer.to_matrix("abbba")))
print("Palindrome", model(tokenizer.to_matrix("abbbba")))
print("Palindrome", model(tokenizer.to_matrix("dfjdsfkrejfarkja" + "dfjdsfkrejfarkja"[::-1])))
print("Not Palindrome",model(tokenizer.to_matrix("abbbca")))
print("Not Palindrome", model(tokenizer.to_matrix("dfjdsfkrejfarkja" + "dfjdsfkrejfarkja"[::-1] + "asd")))</code></pre>

<div style="display:flex; ;justify-content: center;align-items: center;flex-direction: column;">
    <img src="/assets/palindrome-11.png" alt="Output of Training" style="width: 80%">
    <small>Extrapolation</small>
</div>


<h2>Thoughts / Analysis</h2>
<p>
    This article is primarily just a thought experiment to brainstorm how we can merge discrete program search with deep learning algorithms.
    It was inspired by the idea of combining program synthesis and deep learning from the ARC challenge [4].
</p>
<p>To extend this idea further, we would need to try and come up with a network that could take input / output pairs of lots of different rule 
    base algorithms and do the following:
</p>
<ol>
    <li>1. Create a neural network that can solve the problem with highly regularized / discrete "intermediate" layers</li>
    <li>2. After the model is trained, quantize the intermediate layers.</li>
    <li>3. Use discrete rule mining or program synthesis to extrapolate the learned algorithm to new lengths and permutations.</li>
    <li>4. Apply the rule to the entire intermediate matrix or data strucutre to extrapolate.</li>
</ol>
<p>
    Hopefully this article has been an interesting thought expieriment. I will hopefully apply this idea to other problems in the future and
    see if there is any merit to it. You can find the code for this project on my <a target="_blank" href="https://github.com/kevinpdev/kevinpdev.github.io/blob/main/notebooks/nn-palindrome.ipynb">github</a>.
</p>

<h2>Sources</h2>
<ul>
    <li>[1] <a target="_blank" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5002356">Solving Mathematical Problems using Large Language Models: a Survey</a></li>
    <li>[2] <a target="_blank" href="https://domluna.com/blog/gpt-math#needle-in-a-haystack">Needle In a Haystack</a></li>
    <li>[3] <a target="_blank" href="https://arxiv.org/pdf/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
    <li>[4] <a target="_blank" href="https://arcprize.org/arc">ARC Prize</a></li>
</ul>