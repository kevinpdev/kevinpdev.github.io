<!--TEMPLATE:blog.html-->

<h1>Teaching Neural Networks Token Based Arithmetic</h1>
<p>
    This article aims to explore some of the work that has been done on teaching arithmetic to transformers.
    LLMs have been known to struggle with arithmetic performing basic arithmetic operations reliably [1]. I'd  like
    to explore why and run some experiments to give some insight into how we can improve the performance of LLMs on arithmetic tasks.
</p>



<h2>Sources</h2>
<ul>
    <li>[1] <a target="_blank" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5002356">Solving Mathematical Problems using Large Language Models: a Survey</a></li>
    <li>[1] <a target="_blank" href="https://arxiv.org/pdf/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>

</ul>